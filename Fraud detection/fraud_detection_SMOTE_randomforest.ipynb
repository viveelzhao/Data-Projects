{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle('cleaned_data.pkl')\n",
    "y_label = pd.read_pickle('label.pkl')\n",
    "feature_columns = data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaling = preprocessing.StandardScaler()\n",
    "data_scaled = scaling.fit_transform(data_df[feature_columns])\n",
    "scaled_X_train, scaled_X_test, scaled_y_train, scaled_y_test = train_test_split(data_scaled,y_label,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train test splitting\n",
    "scaled_X_train, scaled_X_test, scaled_y_train, scaled_y_test = train_test_split(data_scaled,y_label,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 42)\n",
    "X_res,y_res = sm.fit_sample(scaled_X_train,scaled_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuned_parameters = [{'n_estimators': [20,40,80]}]\n",
    "scores = ['roc_auc']\n",
    "\n",
    "for score in scores:\n",
    "    print \"# Tuning hyper-parameters for %s\" % score\n",
    "    print \n",
    "\n",
    "    clf = GridSearchCV(RF, tuned_parameters, cv=5,\n",
    "                       scoring = score,verbose=True)\n",
    "    clf.fit(X_res, y_res)\n",
    "\n",
    "    print \"Best parameters set found on development set:\",clf.best_params_\n",
    "    print\n",
    "    print \"Grid scores on development set:\"\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print \"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params)\n",
    "    print\n",
    "\n",
    "    print \"Detailed classification report:\"\n",
    "    print \n",
    "    print \"The model is trained on the full development set.\"\n",
    "    print \"The scores are computed on the full evaluation set.\"\n",
    "    print \n",
    "    y_true, y_pred,y_prob = y_test, clf.predict(X_test),clf.predict_proba(X_test)\n",
    "    print classification_report(y_true, y_pred)\n",
    "    print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_tune_results = pd.DataFrame({'num_trees': [20,40,80],\n",
    "                                'test_score':clf.cv_results_['mean_test_score'],\n",
    "                                'train_score':clf.cv_results_['mean_train_score']}\n",
    "                              )\n",
    "RF_tune_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RF_feature_rank(model,topk=15):\n",
    "    # get feature names\n",
    "    feature_names = X_train.columns\n",
    "    # get feature average score\n",
    "    feature_mean_score = model.best_estimator_.feature_importances_\n",
    "    # get feature score (list of array)\n",
    "    scores = [tree.feature_importances_ for tree in model.best_estimator_.estimators_]\n",
    "    # change to a numpy array \n",
    "    scores_array = np.array(scores)\n",
    "    # calculate standard deviation for each feature (column direction)\n",
    "    score_std = np.std(scores_array, axis=0)\n",
    "    \n",
    "    # create a dataframe\n",
    "    feature_df = pd.DataFrame({'feature_names':feature_names,\n",
    "                               'feature_score':feature_mean_score,\n",
    "                               'score_std':score_std})\n",
    "    # get most siginificant features \n",
    "    top_features = feature_df.sort_values('feature_score').iloc[-topk:]\n",
    "    # generate bar plot\n",
    "    fig,ax=plt.subplots()\n",
    "    ax = top_features['feature_score'].plot.barh(xerr=top_features['score_std'],title='feature_importances')\n",
    "    ax.set_xlabel('feature score')\n",
    "    ax.set_ylabel('features')\n",
    "    ax.set_yticklabels(top_features['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_feature_rank(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(ytest,ypredict):\n",
    "    cm =confusion_matrix(ytest, ypredict)\n",
    "    row_sums = cm.astype('float').sum(axis=1,keepdims=True)\n",
    "    RF_cm = cm / row_sums\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    ax = sns.heatmap(RF_cm,annot=True)\n",
    "    ax.set(xlabel='Predicted Label',ylabel='True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_binary_ROC(ytest,yprob):\n",
    "    fpr,tpr,_= roc_curve(ytest,yprob)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr,tpr,color='darkorange',lw=2,label='ROC curve(area = %0.2f)' %roc_auc)\n",
    "    plt.plot([0,1],[0,1],color='navy',lw=2,linestyle='--')\n",
    "    plt.xlim([-0.05,1.0])\n",
    "    plt.ylim([0.0,1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc='lower right')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true,y_pred)\n",
    "plot_binary_ROC(y_true,y_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rocDF(ytrue, yprob):\n",
    "    df = pd.DataFrame()\n",
    "    names = ['threshold', 'precision', 'recall']\n",
    "    data_n = float(ytrue.shape[0])\n",
    "    for prob in np.linspace(0,1,101):\n",
    "        y_prob_positive = yprob >= prob\n",
    "        correct = np.extract(ytrue == y_prob_positive, ytrue)\n",
    "        wrong = np.extract(ytrue != y_prob_positive, ytrue)\n",
    "        tp, tn, fp, fn = [np.sum(correct==1)/data_n,\n",
    "                          np.sum(correct==0)/data_n, \n",
    "                          np.sum(wrong==0)/data_n,\n",
    "                          np.sum(wrong==1)/data_n\n",
    "                         ]\n",
    "        result = [prob, tp/(tp+fn), tp/(tp+fp)]\n",
    "        temp = pd.DataFrame([result], columns=names)\n",
    "        df = pd.concat([df, temp])\n",
    "    return df\n",
    "roc_df = rocDF(y_true, y_prob[:,1])     \n",
    "roc_df\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([0,1])\n",
    "ax.plot(roc_df['recall'], roc_df['precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
