{"cells":[{"cell_type":"markdown","source":["## New York City Bike\n\"Citi Bike is the nation's largest bike share program, with 10,000 bikes and 600 stations across Manhattan, Brooklyn, Queens and Jersey City. It was designed for quick trips with convenience in mind, and itâ€™s a fun and affordable way to get around town.\"\nSelected questions that were explored in this analysis:   \n(1) Given the latitude and longitude of each stop, what is the average distance per trip.  \n(2) What is the fraction of events that bikes are removed from stations for maintenance, by looking at the pickup location of the next trip and the dropoff location of the previous trip for each bike.   \n(3) Find stations that have abnomally high usage compared to the other stations at different hour of the day."],"metadata":{}},{"cell_type":"code","source":["data_links=['https://s3.amazonaws.com/tripdata/201501-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201502-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201503-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201504-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201505-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201506-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201507-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201508-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201509-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201510-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201511-citibike-tripdata.zip',\n           'https://s3.amazonaws.com/tripdata/201512-citibike-tripdata.zip']\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/NYC bikes/\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# download tables to databricks\nimport os\nfrom urlparse import urlparse\nfrom requests import Session\nfrom io import BytesIO\nimport zipfile\nimport tempfile\nfrom urlparse import urlparse\n    \ndef download_to_spark(URL_link, unzip=True):\n  session = Session()\n  tmp = BytesIO()\n  f = session.get(URL_link, headers={'User-Agent': 'Databricks'}, stream=True)\n  for chuck in f.iter_content(chunk_size=1024):\n    if chuck:\n      tmp.write(chuck)\n  tmp.seek(0)\n  \n  if zipfile.is_zipfile(tmp) and unzip:\n    zf = zipfile.ZipFile(tmp)   \n    member = zf.namelist()[0]\n  with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='csv') as t:\n    t.write(zf.read(member))\n    t.close()\n    dbutils.fs.cp('file://{0}'.format(t.name), os.path.join('/NYC bikes/', member))\n    os.unlink(t.name)\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["for link in data_links:\n  download_to_spark(link)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# check whether download is successful. \ndisplay(dbutils.fs.ls(\"/NYC bikes/\"))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.types import StructType, IntegerType, StringType, FloatType, StructField\n\nbike_data_features= ['tripduration',\n                   'starttime',\n                   'stoptime',\n                   'start station id',\n                   'start station name',\n                   'start station latitude',\n                   'start station longitude',\n                   'end station id',\n                   'end station name',\n                   'end station latitude',\n                   'end station longitude',\n                   'bikeid',\n                   'usertype',\n                   'birth year',\n                   'gender']\nbike_data_dtypes = [IntegerType(),\n                    StringType(),\n                    StringType(),\n                    IntegerType(),\n                    StringType(),\n                    FloatType(),\n                    FloatType(),\n                    IntegerType(),\n                    StringType(),\n                    FloatType(),\n                    FloatType(),\n                    IntegerType(),\n                    StringType(),\n                    IntegerType(),\n                    IntegerType()\n                   ]\n\nbike_data_schema = StructType([StructField(i,j,True) for i, j in zip(bike_data_features,bike_data_dtypes)])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# combine all the tables into one dataframe\ncount = 0\nfor file in dbutils.fs.ls('/NYC bikes/'):\n  file_path = file.path\n  if count ==0:\n    raw_bike_df = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=True).schema(bike_data_schema).load(file_path)\n  else:\n    tmp_df = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=True).schema(bike_data_schema).load(file_path)\n    raw_bike_df = raw_bike_df.unionAll(tmp_df)\n  count +=1\n  tmp_df=[]\nraw_bike_df.cache() "],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["print raw_bike_df.count()\nprint raw_bike_df.distinct().count()\nprint raw_bike_df.select('bikeid').distinct().count()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(raw_bike_df.head(2))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# check every numerical columns for outliers.\nraw_bike_df.describe(['tripduration','start station latitude','start station longitude','end station latitude','end station longitude']).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["The maximum trip duration is more than 1629 hours. But even for annual membership (https://www.citibikenyc.com/pricing), riders will be charged for additional dollars if keeping the bikes for more than 45 minutes in a single ride. \nSo let's assume that entries that have duration longer than 24 hours are errors."],"metadata":{}},{"cell_type":"code","source":["display(raw_bike_df.filter(raw_bike_df.tripduration>86400).collect())"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["(1) Let's look at median trip duration"],"metadata":{}},{"cell_type":"code","source":["tripduration = raw_bike_df.select('tripduration').collect()\nprint 'median trip duration is %.10f' %(np.median(tripduration))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Most people use Citi bikes for short trips."],"metadata":{}},{"cell_type":"markdown","source":["(2) The fraction of rides start and end at the same station."],"metadata":{}},{"cell_type":"code","source":["return_to_same_stop_df = raw_bike_df.filter(raw_bike_df['start station id']==raw_bike_df['end station id'])\nratio_to_the_same_stop = float(return_to_same_stop_df.count())/float(raw_bike_df.count())\nprint ratio_to_the_same_stop"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Only 2% of the bikes are returned to the same stops."],"metadata":{}},{"cell_type":"markdown","source":["(3) The standard deviation of the number of stations visited by a bike.\nThe station visited by a bike could be the start station or the end station. So first I create a table by selecting \nbikeid and start station id, and another table by selecting bikeid and end station id. The I union two tables, and count\nthe unique stations, grouped by bikeid."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\nbike_start_df = raw_bike_df.select('bikeid','start station id').withColumnRenamed('start station id', 'station_visited')\nbike_end_df = raw_bike_df.select('bikeid','end station id').withColumnRenamed('end station id','station_visited')\nbike_stations_df = bike_start_df.unionAll(bike_end_df)\nbike_stations_df = bike_stations_df.distinct().groupBy('bikeid').count()\nstd_stations_visited = bike_stations_df.agg(F.stddev('count'))\nprint std_stations_visited.collect()[0][0]\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["The average length, in kilometers, of a trip. Assume trips follow great circle arcs from the start station to the end station. Ignore trips that start and end at the same station, as well as those with obviously wrong data."],"metadata":{}},{"cell_type":"code","source":["geometry_df = raw_bike_df.filter(~(raw_bike_df['start station id']==raw_bike_df['end station id']))\ngeometry_df = geometry_df.select('tripduration',\n                                 geometry_df['start station latitude'].alias('s_lat'), \n                                 geometry_df['start station longitude'].alias('s_lon'),\n                                 geometry_df['end station latitude'].alias('e_lat'),\n                                 geometry_df['end station longitude'].alias('e_lon'))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from math import sin, cos, sqrt, atan2, radians\ndef great_circle_distance(lat_s,lon_s,lat_e,lon_e):\n  earth_radius = 6371.0088\n  lat1=radians(lat_s)\n  lon1=radians(lon_s)\n  lat2=radians(lat_e)\n  lon2=radians(lon_e)  \n  \n  lat_diff=lat2-lat1\n  lon_diff=lon2-lon1\n  \n  a = sin(lat_diff / 2)**2 + cos(lat1) * cos(lat2) * sin(lon_diff / 2)**2\n  c = 2 * atan2(sqrt(a), sqrt(1 - a))\n  distance = earth_radius * c\n  return distance\n\nx = great_circle_distance(52.2296756,21.0122287, 52.406374,16.9251681)\nprint type(x)\n\ndistance_udf = udf(great_circle_distance,FloatType())\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["geometry_df = geometry_df.withColumn('distance', distance_udf('s_lat','s_lon','e_lat','e_lon'))\ngeometry_df = geometry_df.withColumn('speed', (geometry_df.distance/geometry_df.tripduration)*3600.0) # calculate the speed km/hr"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["geometry_df.describe('speed').show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["The hourly speed for bike should be less than 40 km/hr. There might be errors in the data."],"metadata":{}},{"cell_type":"code","source":["geometry_df.filter(geometry_df.speed >40).count()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["geometry_corrected = geometry_df.filter(geometry_df.speed <40)\naverage_distance = geometry_corrected.agg(F.mean('distance')).collect()[0][0]\nprint average_distance"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["The Average length of the trip is 1.76 km."],"metadata":{}},{"cell_type":"markdown","source":["(5) Calculate the average duration of trips for each month in the year. We should see a seasonal pattern."],"metadata":{}},{"cell_type":"code","source":["from datetime import datetime\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\n\ndef extract_month(start_time):\n  try:\n    t1 = datetime.strptime(start_time, '%m/%d/%Y %H:%M:%S')\n  except ValueError:\n    t1 = datetime.strptime(start_time, '%m/%d/%Y %H:%M')\n  return t1.month\nextract_month_udf = udf(extract_month, IntegerType())"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["duration_df = raw_bike_df.select('tripduration',extract_month_udf('starttime').alias('month'))\nduration_mean_df = duration_df.groupBy('month').avg('tripduration')\nduration_spread =duration_mean_df.agg(F.max('avg(tripduration)')-F.min('avg(tripduration)')).first()[0]\nprint duration_spread"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["duration_mean_df.sort('avg(tripduration)',ascending = False).show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["It is very surprising that on average, the October and September has longest tripduration durations. I would assume July and August are the busy seasons."],"metadata":{}},{"cell_type":"markdown","source":["(6) Next we'll look at the how busy the stations are. Let us define the hourly usage fraction of a station to be the fraction of all rides starting at that station that leave during a specific hour."],"metadata":{}},{"cell_type":"code","source":["def extract_hour(start_time):\n  try:\n    t1 = datetime.strptime(start_time, '%m/%d/%Y %H:%M:%S')\n  except ValueError:\n    t1 = datetime.strptime(start_time, '%m/%d/%Y %H:%M')\n  return t1.hour\nextract_hour_udf = udf(extract_hour, IntegerType())"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["raw_bike_df.select('bikeid').distinct().count()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Let's look at overall bike usage rate\nhourly_bike_df = raw_bike_df.select('start station id',extract_hour_udf('starttime').alias('hour'))\nhourly_df = hourly_bike_df.groupBy('hour').count()\ntotal_uses = float(hourly_df.agg(F.sum('count')).collect()[0][0])\nhourly_df = hourly_df.select('hour','count',(hourly_df['count']/total_uses).alias('system_ratio'))\nhourly_df = hourly_df.withColumnRenamed('count','hourly_count')"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["display(hourly_df.sort('system_ratio', ascending=False).collect())"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Bikes have highest usage around rush hours in the morning or in the late afternoon."],"metadata":{}},{"cell_type":"code","source":["station_df = hourly_bike_df.groupBy(['start station id','hour']).count().sort('start station id','hour')\nstation_hourly_df = station_df.groupBy('start station id').sum('count')\ndisplay(station_hourly_df.take(2))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["station_fraction_df = station_df.join(station_hourly_df, 'start station id', 'left_outer').join(hourly_df,'hour','left_outer')\n#display(station_fraction_df)\nstation_fraction_df = station_fraction_df.withColumn('station hourly usage fraction',\n                                                     station_fraction_df['count']/station_fraction_df['sum(count)'])"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["station_fraction_df = station_fraction_df.withColumn('over load ratio',\n                                                     station_fraction_df['station hourly usage fraction']/station_fraction_df['system_ratio'])\nover_load_station = station_fraction_df.sort('over load ratio',ascending=False)\nover_load_station.first()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["For example, 3 o'clock in the morning station 3103 has abnormal usage rate compared to the all other stations."],"metadata":{}},{"cell_type":"markdown","source":["(7) There are two types of riders: \"Customers\" and \"Subscribers.\" Customers buy a short-time pass which allows 30-minute rides. Subscribers buy yearly passes that allow 45-minute rides. What fraction of rides exceed their corresponding time limit?"],"metadata":{}},{"cell_type":"code","source":["timelimit_short = 30*60\ntimelimit_long = 45*60\nsubscriber_df = raw_bike_df.filter(raw_bike_df.usertype =='Subscriber')\nsubscriber_exceed_num = subscriber_df.filter(subscriber_df.tripduration > timelimit_short).count()\nsubscriber_exceed_ratio = float(subscriber_exceed_num)/subscriber_df.count()\n\ncustomers_df = raw_bike_df.filter(raw_bike_df.usertype=='Customer')\ncustomers_exceed_num = customers_df.filter(customers_df.tripduration > timelimit_long).count()\ncustomers_exceed_ratio = float(customers_exceed_num)/customers_df.count()\n\nprint subscriber_exceed_ratio,customers_exceed_ratio"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["The subscribers have smaller chance to exceed the single ride limit."],"metadata":{}},{"cell_type":"markdown","source":["(8). Most of the time, a bike will begin a trip at the same station where its previous trip ended. Sometimes a bike will be moved by the program, either for maintenance or to rebalance the distribution of bikes. Let's find out the average number of times a bike is moved during this period, as detected by seeing if it starts at a different station than where the previous ride ended?"],"metadata":{}},{"cell_type":"code","source":["# create index id column \"timeid\",create two tables, the first table contains bikeid, start station id, timeid and the second table contains bikeid, end station id, timeid\n# For each table, Substrate 1 from \"timeid\" for the first table\n# join two tables by timeid and bikeid\nfrom pyspark.sql import Row\ndef mergeIndex(onerow):\n  return Row(**dict(onerow[0].asDict().items() + [('index', onerow[1])]))\nordered_bike_df = (raw_bike_df.select('bikeid','starttime','start station id','end station id')\n                             .withColumnRenamed(\"start station id\",\"start_station_id\")\n                             .withColumnRenamed(\"end station id\",\"end_station_id\")\n                             .orderBy('bikeid','starttime'))\nbike_index_df = ordered_bike_df.rdd.zipWithIndex().map(mergeIndex).toDF()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["display(bike_index_df.take(5))"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["start_df = bike_index_df.select('index', 'bikeid', 'start_station_id')\nend_df = bike_index_df.selectExpr('index+1', 'bikeid', 'end_station_id').withColumnRenamed('(index + 1)', 'index')\nshifted_df = start_df.join(end_df, on=['bikeid', 'index']).cache()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["diff_df = shifted_df.filter(shifted_df.start_station_id != shifted_df.end_station_id).cache()\ndiff_count = diff_df.count()\ntotal_count = bike_index_df.count()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["print \"The probability that the bike gets removed for maintenance is {0:.2f}.\".format(diff_count / float(total_count))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["11% of the time, the bikes were taken for maintainence."],"metadata":{}},{"cell_type":"code","source":["# 2nd method using window function\nfrom pyspark.sql.functions import lag, col\nfrom pyspark.sql.window import Window\nordered_bike_df = raw_bike_df.orderBy('bikeid','starttime')\ndisplay(ordered_bike_df)\nw = Window().partitionBy('bikeid').orderBy('starttime')\naligned_bike_df = ordered_bike_df.select(\"*\", lag('end station id').over(w).alias(\"stop station id\")).orderBy('bikeid','starttime')\ndisplay(aligned_bike_df)\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["bike_replace_df = aligned_bike_df.dropna().filter(aligned_bike_df['start station id']!=aligned_bike_df['stop station id'])\nreplace_ratio = float(bike_replace_df.count())/raw_bike_df.count()\nprint replace_ratio"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":59}],"metadata":{"name":"New York Citi Bikes","notebookId":2765938962811128},"nbformat":4,"nbformat_minor":0}
