{"cells":[{"cell_type":"markdown","source":["###### 0. Loading libraries"],"metadata":{}},{"cell_type":"code","source":["# -*- encode:utf-8 -*-\nimport re\nimport time\nimport requests\nimport string\nfrom datetime import date\nimport os\nfrom io import BytesIO\nfrom pyspark.sql import SQLContext, Row\nimport pyspark.sql.functions as sqlFunction\nfrom pyspark.sql.types import *\nfrom fnmatch import fnmatch\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["###### 1. Reading park info file: read records from uploaded table"],"metadata":{}},{"cell_type":"code","source":["parkinfo_location = '/FileStore/tables/9kjsmmkl1478305508067/parksinfo4.csv'\n\npark_df_features = ['null',\n                 'park_id',\n                 'html',\n                 'name',\n                 'city',\n                 'state',\n                 'Excellent',\n                 'Very_good',\n                 'Average',\n                 'Poor',\n                 'Terrible',\n                 'Families',\n                 'Couples',\n                 'Friends',\n                 'Solo',\n                 'Business',\n                 'Spring',\n                 'Summer',\n                 'Fall',\n                 'Winter']\npark_df_dtypes =[FloatType(),\n                    FloatType(),\n                    StringType(),\n                    StringType(),\n                    StringType(),\n                    StringType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType(),\n                    FloatType()]\npark_df_schema = StructType([StructField(i, j, True) for i, j in zip(park_df_features, park_df_dtypes)])  \nparks_df=sqlContext.read.format(\"com.databricks.spark.csv\").options(header=True).schema(park_df_schema).load(parkinfo_location).drop('null')\nparks_df.cache()\ndisplay(parks_df)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import urllib\nACCESS_KEY = \"\"\nSECRET_KEY = \"\"\nENCODED_SECRET_KEY = urllib.quote(SECRET_KEY, \"\")\nAWS_BUCKET_NAME = \"parks101\"\nMOUNT_NAME = \"parks_info_review\"\ntry:\n  dbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  print 'AWS bucket already mounted.'"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["review_df_features= ['index',\n                     'review_index',\n                     'comment',\n                     'date',\n                     'park_id',\n                     'stars',\n                     'title',\n                     'reviewer_level',\n                     'reviewer']\nreview_df_dtypes = [IntegerType(),\n                    IntegerType(),\n                    StringType(),\n                    StringType(), \n                    FloatType(), \n                    FloatType(),\n                    StringType(),\n                    FloatType(),\n                    StringType()]\nreview_df_schema = StructType([StructField(i, j, True) for i, j in zip(review_df_features, review_df_dtypes)])\nparks_review_df=(sqlContext.read\n                 .format(\"com.databricks.spark.csv\")\n                 .schema(review_df_schema)\n                 .options(header=True)\n                 .load('dbfs:/mnt/parks_info_review/reviews_raw.csv')).drop('review_index')\ndisplay(parks_review_df)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["##### 2. Data Cleaning\n\n* Correct state name\n* add state name abbreviation (for plotting in databricks)"],"metadata":{}},{"cell_type":"markdown","source":["###### 2.1 Data cleaning: finding alias of state names"],"metadata":{}},{"cell_type":"code","source":["states = ['Mississippi', 'Oklahoma', 'Delaware', 'Minnesota', 'Illinois', 'Arkansas', 'New Mexico', 'Indiana', 'Louisiana', 'Texas', 'Wisconsin', 'Kansas', 'Connecticut', 'California', 'West Virginia', 'Georgia', 'North Dakota', 'Pennsylvania', 'Alaska', 'Missouri', 'South Dakota', 'Colorado', 'New Jersey', 'Washington', 'New York', 'Nevada', 'Washington DC', 'Maryland', 'Idaho', 'Wyoming', 'Arizona', 'Iowa', 'Michigan', 'Utah', 'Virginia', 'Oregon', 'Montana', 'New Hampshire', 'Massachusetts', 'South Carolina', 'Vermont', 'Florida', 'Hawaii', 'Kentucky', 'Rhode Island', 'Nebraska', 'Ohio', 'Alabama', 'North Carolina', 'Tennessee', 'Maine']\nwrong_state_name=parks_df.select('city', 'state').filter(~ parks_df['state'].isin(states)).distinct()\ndisplay(wrong_state_name)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["###### 2.2a Data cleaning: stardardize state names"],"metadata":{}},{"cell_type":"code","source":["def state_correct(city, state_name):\n  corrected_name = state_name\n  wrongspell = {'Columbia': 'Washington DC',\n                'Ha': 'Hawaii',\n                'Ma': 'Maine',\n                'Missi': 'Mississippi',\n                'C': 'California',\n                'Califor': 'California',\n                'Cali': 'California',\n                'Calif': 'California',\n                'Califo': 'California',\n                'Califor': 'California',\n                'Wy': 'Wyoming',\n                'Reg': 'Pennsylvania',\n                'Nationa': 'Alaska',\n                'Georgi': 'Georgia',\n                'Carolin': 'Carolina',\n                'Fran': 'California'}\n  if corrected_name in wrongspell:\n    corrected_name = wrongspell[corrected_name]\n  prefix = city.split('_')[-1]\n  if prefix in ['South', 'North', 'West', 'New', \"Rhode\"]:\n    corrected_name = prefix + ' ' + corrected_name\n  return corrected_name\n  \nstate_correct_udf=udf(state_correct, StringType())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["###### 2.2b Data cleaning: replace'_' with ' ' for park names"],"metadata":{}},{"cell_type":"code","source":["def park_name(pname):\n  return re.sub('_',' ',pname)\npark_name_udf = udf(park_name,StringType())"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["parks_df = parks_df.withColumn('total_visit',(parks_df['Spring']+parks_df['Summer']+parks_df['Fall']+parks_df['Winter']))\nparks_df = parks_df.withColumn('state', state_correct_udf('city', 'state'))\nparks_df_clean=parks_df.withColumn('name',park_name_udf('name'))\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(parks_df_clean.select('name','state','total_visit').sort('total_visit',ascending=False))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["The most popular parks are in or around big cities."],"metadata":{}},{"cell_type":"markdown","source":["###### 2.3 Data cleaning: create state name abbrevition column for geometric display\ndatabricks display option map only takes state name abbreviation, so conversion from state name to its abbreviation is needed."],"metadata":{}},{"cell_type":"code","source":["def state_abbrev(state_name):\n  state_table={'Alabama':'AL',\n                'Alaska':'AK',\n                'Arizona':'AZ',\n                'Arkansas':'AR',\n                'California':'CA',\n                'Colorado':\t'CO',\n                'Connecticut':'CT',\n                'Delaware':'DE',\n                'Florida':'FL',\n                'Georgia':'GA',\n                'Hawaii':'HI',\n                'Idaho':'ID',\n                'Illinois':'IL',\n                'Indiana':'IN',\n                'Iowa':'IA',\n                'Kansas':'KS',\n                'Kentucky':'KY',\n                'Louisiana':'LA',\n                'Maine':'ME',\n                'Maryland':'MD',\n                'Massachusetts':'MA',\n                'Michigan':'MI',\n                'Minnesota':'MN', \n                'Mississippi':'MS',\n                'Missouri':'MO',\n                'Montana':'MT',\n                'Nebraska':'NE',\n                'Nevada':'NV',\n                'New Hampshire':'NH',\n                'New Jersey':'NJ',\n                'New Mexico':'NM',\n                'New York':'NY',\n                'North Carolina':'NC',\n                'North Dakota':'ND',\n                'Ohio':'OH',\n                'Oklahoma':'OK',\n                'Oregon':'OR',\n                'Pennsylvania':'PA',\n                'Rhode Island':'RI',\n                'South Carolina':'SC',\n                'South Dakota':'SD',\n                'Tennessee':'TN',\n                'Texas':'TX',\n                'Utah':'UT',\n                'Vermont':'VT',\n                'Virginia':'VA',\n                'Washington':'WA',\n                'Washington DC': 'DC',\n                'West Virginia':'WV',\n                'Wisconsin':'WI',\n                'Wyoming':'WY'}\n  try:\n    s_name=state_table[state_name]\n  except KeyError:\n    s_name=state_name\n  return s_name\nstate_abbrev_udf=udf(state_abbrev,StringType())\nparks_df_abb=parks_df_clean.withColumn('state_abb',state_abbrev_udf(parks_df_clean['state']))\nparks_df_abb.cache()\ndisplay(parks_df_abb)\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["def float2int(num):\n  if isinstance(num, float):\n    return int(num)\n  else:\n    return num\n\ndef str2date(onestr):\n    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n    try:\n      segs = re.search('(January|February|March|April|May|June|July|August|September|October|November|December).*?(\\d{1,2}).*?(\\d{4})', onestr)\n      month, day, year = segs.group(1), segs.group(2), segs.group(3)\n      qdate = date(int(year), months.index(month) + 1, int(day))\n    except:\n      return None\n    return qdate\n  \ndef wordvec(onestr):\n  newstr = re.sub('[^a-z0-9 ]', '', onestr.lower()).split()\n  return newstr\n  \nfloat2intudf = udf(float2int, IntegerType())\nstr2dateudf = udf(str2date, DateType())\nwordvecudf = udf(wordvec, ArrayType(StringType()))\n\n\nreview_df = parks_review_df.select(#float2intudf('index').alias('index'),\n                                               float2intudf('park_id').alias('park_id'), \n                                               #'name',\n                                               #'state_abb',\n                                               #float2intudf('review_index').alias('review_index'),\n                                               'reviewer',\n                                               #float2intudf('reviewer_level').alias('reviewer_level'),\n                                               str2dateudf('date').alias('date'),\n                                               #wordvecudf('title').alias('title'),\n                                               #wordvecudf('comment').alias('comment'),\n                                               'title',\n                                               'comment',\n                                               'stars').distinct().cache()\ndisplay(review_df)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["###### 2.4 Save cleaned park info data to HDFS"],"metadata":{}},{"cell_type":"code","source":["save_switch = False\nif save_switch:\n  parks_df_abb.write.format(\"com.databricks.spark.csv\").option('header','true').mode('overwrite').save('/Wei data/parks_info_clean.csv')\ndisplay(dbutils.fs.ls('Wei data/parks_info_clean.csv'))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["dbutils.fs.ls('/Wei data/')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from boto.s3.connection import S3Connection\nfrom boto.s3.key import Key\n\nACCESS_KEY = \"\"\nSECRET_KEY = \"\"\nBUCKET_NAME = 'parks101'\nfile_name = 'park_info_clean.csv'\ns3_handle = S3Connection(ACCESS_KEY, SECRET_KEY)\nbucket_handle = s3_handle.get_bucket(BUCKET_NAME)\nnew_file_handle = bucket_handle.new_key(file_name)\nnew_file_handle.set_contents_from_string(parks_df_abb.toPandas().to_csv())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["for i in bucket_handle.list():\n  print i.name"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["def read_in_clean_df(filepath):\n  park_df_features = ['null',\n                   'park_id',\n                   'html',\n                   'name',\n                   'city',\n                   'state',\n                   'Excellent',\n                   'Very_good',\n                   'Average',\n                   'Poor',\n                   'Terrible',\n                   'Families',\n                   'Couples',\n                   'Friends',\n                   'Solo',\n                   'Business',\n                   'Spring',\n                   'Summer',\n                   'Fall',\n                   'Winter',\n                   'total_visit',\n                   'state_abb']\n  park_df_dtypes =[FloatType(),\n                      FloatType(),\n                      StringType(),\n                      StringType(),\n                      StringType(),\n                      StringType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      FloatType(),\n                      StringType()]\n  park_df_schema = StructType([StructField(i, j, True) for i, j in zip(park_df_features, park_df_dtypes)])  \n  df=sqlContext.read.format(\"com.databricks.spark.csv\").options(header=True).schema(park_df_schema).load(filepath)\n  df.cache()\n  return df\n\nswitch = False\nif switch:\n  parkinfo_location = '/Wei data/parks_info_clean.csv'\n  parks_df_abb = read_in_clean_df(parinfo_location)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["###### 3.1 Exploration: Parks and visitings per state count"],"metadata":{}},{"cell_type":"code","source":["# this is to remove 'sum()' from 'sum(col_name)' in an aggregated dataframe\ndef recolumn_grouped_df(df):\n  cols = df.columns\n  index=[]\n  for i, col in enumerate(cols):\n    try:\n      index.append(re.search('\\((.*?)\\)', col).group(1))\n    except AttributeError:\n      index.append(col)\n  return df.toDF(*index)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["###### 3.1.1 Group by park"],"metadata":{}},{"cell_type":"code","source":["visit_threshold = 0\nparks_sum_df=recolumn_grouped_df(parks_df_abb.filter(parks_df_abb['total_visit']>visit_threshold).groupBy('state_abb').sum()).cache()\ndisplay(parks_sum_df)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["###### 3.1.1 First count the visits in each state"],"metadata":{}},{"cell_type":"code","source":["display(parks_sum_df.select('state_abb', 'total_visit'))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["The most attractive states are California, Florida, and New York."],"metadata":{}},{"cell_type":"markdown","source":["###### 3.1.2 Result dominated by recreation sites. Filter only national parks"],"metadata":{}},{"cell_type":"code","source":["def isnationalpark(park_name):\n  return 'National Park' in park_name\nisnationalpark_udf = udf(isnationalpark, BooleanType())"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["nationalparks_df = parks_df_abb.filter(isnationalpark_udf('name')).cache()\ndisplay(nationalparks_df)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["visit_threshold = 0\nnationalparks_sum_df=recolumn_grouped_df(nationalparks_df.filter(nationalparks_df['total_visit']>visit_threshold).groupBy('state_abb').sum()).cache()\ndisplay(nationalparks_sum_df.select('state_abb','total_visit'))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Arizona, Utah, and Florida has the top three visitors for national park. Too bad, many states don't have national parks."],"metadata":{}},{"cell_type":"markdown","source":["###### 3.1.3 How about favorate ratio"],"metadata":{}},{"cell_type":"code","source":["display(nationalparks_sum_df.selectExpr('state_abb', '(Excellent + Very_good) / total_visit'))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["###### 3.1.4 Then how about disfavorate ratio"],"metadata":{}},{"cell_type":"code","source":["display(nationalparks_sum_df.selectExpr('state_abb', '(Poor + Terrible) / total_visit'))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["###### 3.1.5 Break into seasons"],"metadata":{}},{"cell_type":"code","source":["display(nationalparks_sum_df.selectExpr('state_abb', 'Spring / total_visit'))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["display(nationalparks_sum_df.selectExpr('state_abb', 'summer / total_visit'))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["display(nationalparks_sum_df.selectExpr('state_abb', 'Fall / total_visit'))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["display(nationalparks_sum_df.selectExpr('state_abb', 'Winter / total_visit'))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["###### 3.1.6 Seems for winter season there are a few choices"],"metadata":{}},{"cell_type":"code","source":["winter_states = ['TX', 'FL', 'CA', 'SC', 'VA', 'HA', 'AZ', 'OH']\nwinter_nationalparks_sum_df = (nationalparks_df.drop('null')\n                                              .filter(nationalparks_df['state_abb'].isin(winter_states))\n                                              .groupBy(['park_id','name','state_abb'])\n                                              .sum().drop('sum(park_id)'))\nwinter_nationalparks_df = recolumn_grouped_df(winter_nationalparks_sum_df).sort('total_visit',ascending=False)                                           \ndisplay(winter_nationalparks_df)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["winter_nationalparks_df.take(5)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["winter_nationalparks_df.columns"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["###### 3.1.7 Visualize the results\n* Approval score\n* Type of visit"],"metadata":{}},{"cell_type":"code","source":["winterpark_data = winter_nationalparks_df.collect()\nwinterpark_dic = dict()\nfor c in winter_nationalparks_df.columns:\n  winterpark_dic[c] = [row[c] for row in winterpark_data]  \nvisit_types = ['Families',\t'Couples',\t'Friends',\t'Solo',\t'Business']\nrating_levels = ['Excellent',\t'Very_good',\t'Average',\t'Poor',\t'Terrible']\npark_names = winterpark_dic['name']\ncolors = ['r', 'g', 'b', 'c', 'm']\nnum_parks = len(park_names)\nnum_visit_types = len(visit_types)\nnum_rating_levels = len(rating_levels)\n\nfig = plt.figure(figsize=(12,9))\nwidth = 1.4\n\nax = plt.subplot(211)\nax.set_aspect(150)\nax.set_xlim([0, 10000])\nypos = np.arange(num_parks) * 2\nplots = []\nfor i, vtype in enumerate(visit_types):\n  xpos = winterpark_dic[vtype]\n  #datalen = len(ypos)\n  if not i: #i=0\n    p = plt.barh(ypos, xpos, width, color = colors[i])\n    plots.append(p)\n    xsum = xpos[:]\n  else:\n    p = plt.barh(ypos, xpos, width, color = colors[i], left = xsum)\n    plots.append(p)\n    xsum = [xsum[i] + xpos[i] for i in range(num_parks)]\nplt.legend([p[0] for p in plots], visit_types, prop={'size':8}, bbox_to_anchor=(0.9, 0.4))\nplt.yticks(ypos +width/2., park_names, rotation=0, size=8)\nplt.xlabel('visit counts')\n\n\nax = plt.subplot(212)\nax.set_aspect(150)\nax.set_xlim([0, 10000])\nypos = np.arange(num_parks) * 2\nplots = []\nfor i, level in enumerate(rating_levels):\n  xpos = winterpark_dic[level]\n  if not i:\n    p = plt.barh(ypos, xpos, width, color = colors[i])\n    plots.append(p)\n    xsum = xpos[:]\n  else:\n    p = plt.barh(ypos, xpos, width, color = colors[i], left = xsum)\n    plots.append(p)\n    xsum = [xsum[i] + xpos[i] for i in range(num_parks)]\nplt.legend([p[0] for p in plots], rating_levels, prop={'size':8}, bbox_to_anchor=(0.9, 0.4))\nplt.yticks(ypos +width/2., park_names, rotation=0, size=8)\nplt.xlabel('visit counts')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["###### 3.2 Free memories"],"metadata":{}},{"cell_type":"code","source":["parks_df.unpersist()\nparks_sum_df.unpersist()\nnationalparks_df.unpersist()\nwinter_nationalparks_df.unpersist()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["number_reviewers = review_df.select('reviewer').distinct().count()\nprint \"There are %d reviewers.\" %number_reviewers\n"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["total_reviews=review_df.count()\nprint \"The total reviews are %d.\" %total_reviews"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["park_review_count = review_df.groupby('park_id').count().select('park_id','count').sort('count',ascending=False)\ndisplay(park_review_count)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["review_name_df=review_df.filter(review_df['reviewer']!='unknown').select('park_id','reviewer','stars')"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["grouped_reviewer_df = review_name_df.groupby('reviewer').count().select('reviewer','count')\nreviewer_count_df = grouped_reviewer_df.select('reviewer',grouped_reviewer_df['count'].alias('num of reviews')).sort('num of reviews',ascending=False)\ndisplay(reviewer_count_df.head(5))"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["average_reviews_per_id= review_name_df.groupby('reviewer').count().selectExpr('mean(count)')\nprint average_reviews_per_id.collect()[0][0]"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["from pyspark.sql.functions import monotonicallyIncreasingId\nparks_review_df = review_name_df.withColumn(\"user_id\", monotonicallyIncreasingId()+1).select('user_id','park_id','stars')\ndisplay(parks_review_df)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["seed = 0\n(training_df,validation_df,test_df)=parks_review_df.randomSplit([0.6,0.2,0.2],seed=seed)\ntraining_df.cache()\nvalidation_df.cache()\ntest_df.cache()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\nals=ALS()\nals.setMaxIter(6)\\\n   .setSeed(seed)\\\n   .setRegParam(0.1)\\\n   .setUserCol('user_id')\\\n   .setItemCol('park_id')\\\n   .setRatingCol('stars')\nfrom pyspark.ml.evaluation import RegressionEvaluator\nreg_eval = RegressionEvaluator(predictionCol='prediction',labelCol='stars',metricName='rmse')\ntolerance = 0.03\nranks=[4,8,12]\nerrors=[0,0,0]\nmodels=[0,0,0]\nerr=0\nmin_error=float('inf')\nbest_rank=-1\nfor rank in ranks:\n  als.setRank(rank)\n  model=als.fit(training_df)\n  predict_df=model.transform(validation_df)\n  predicted_star_df=predict_df.filter(predict_df.prediction != float('nan'))\n  error= reg_eval.evaluate(predicted_star_df)\n  errors[err]=error\n  models[err]=model\n  print 'For rank %s the RMSE is %s' %(rank, error)\n  if error < min_error:\n    min_error = error\n    best_rank = err\n  err +=1\nals.setRank(ranks[best_rank])\nprint 'The best model was trained with rank %s' %ranks[best_rank]\nbest_model = models[best_rank]\n\n'''predict test df'''\npredict_df=best_model.transform(test_df)\npredicted_test_df=predict_df.filter(predict_df.prediction != float('nan'))\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\nprint test_RMSE"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["display(parks_df_abb.select('park_id','name','state'))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["'''predict for yourself'''\nfrom pyspark.sql import Row\nmy_user_id=0\nmy_rated_parks =[(my_user_id,5,4.5 ),(my_user_id,8,5.0),(my_user_id,17,3.0),\n                (my_user_id,10,5.0),(my_user_id,39,5.0),(my_user_id,53,2.0),(my_user_id,776,1.0),(my_user_id,1043,2.0),(my_user_id,158,0.5),(my_user_id,909,4.8)] #Balboa Park, Grand Canyon National Park,Mammoth Cave National Park,Acadia National Park,Magic Kindom,Independence Visitor Center,The National WWII Museum,Martin Luther King Jr National Historic Site\n\nmy_parks_df = sqlContext.createDataFrame(my_rated_parks,['user_id','park_id','stars'])\ntraining_with_my_ratings_df = training_df.unionAll(my_parks_df)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["import pyspark.sql.functions as F\nals.setPredictionCol('prediction')\\\n   .setMaxIter(6)\\\n   .setSeed(seed)\\\n   .setUserCol('user_id').setItemCol('park_id').setRatingCol('stars').setRank(ranks[best_rank])\nmy_ratings_model = als.fit(training_with_my_ratings_df)\nmy_rated_park_id=[x[1] for x in my_rated_parks]\nnot_rated_df = parks_df_abb.filter(~parks_df_abb['park_id'].isin(my_rated_park_id))\nmy_not_rated_df=not_rated_df.withColumn('user_id',F.lit(my_user_id))\nraw_predicted_ratings_df = my_ratings_model.transform(my_not_rated_df)\npredicted_ratings_df = raw_predicted_ratings_df.filter(raw_predicted_ratings_df['prediction']!=float('nan')) "],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["recommend_prediction_df=predicted_ratings_df.filter(predicted_ratings_df['prediction']>4.2).sort('Winter','Families',ascending=False)\ndisplay(recommend_prediction_df.select('name','state','Winter','Families','prediction'))"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["not_recommend_prediction_df=predicted_ratings_df.filter(predicted_ratings_df['prediction']<2.0).sort('Winter','Families',ascending=False)\ndisplay(not_recommend_prediction_df.select('park_id','name','state','Winter','Families','prediction'))"],"metadata":{},"outputs":[],"execution_count":68}],"metadata":{"name":"trip  planner exploratory analysis","notebookId":4085745380267540},"nbformat":4,"nbformat_minor":0}
