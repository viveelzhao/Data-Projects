{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 0. Read Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from math import ceil\n",
    "from itertools import product\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Setting global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIT_SWITCH = True\n",
    "DATA_FOLDER = '/Users/Wei.Zhao/Documents/Python code/tripadvisor/'\n",
    "MODEL_FOLDER = '/Users/Wei.Zhao/Documents/Python code/tripadvisor/models/SGD'\n",
    "PARK_ASSIGNMENT_FILE ='park_assignment.csv'\n",
    "CLEAN_PARK_INFO_FILE = 'parks_info_final.csv'\n",
    "CLEAN_REVIEW_FILE = 'reviews_clean.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Read Data from saved cleaned dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_id</th>\n",
       "      <th>review_index</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>reviewer_level</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>cluster_assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>759</td>\n",
       "      <td>0</td>\n",
       "      <td>podrozniczka60</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-09-26</td>\n",
       "      <td>3</td>\n",
       "      <td>helpful with your utah vacation planning</td>\n",
       "      <td>helpful with your utah vacation planning   uta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>759</td>\n",
       "      <td>1</td>\n",
       "      <td>prpatel007</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>5</td>\n",
       "      <td>shame on us americans</td>\n",
       "      <td>shame on us americans   recently i and my wife...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>759</td>\n",
       "      <td>2</td>\n",
       "      <td>Flybob6334580</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-10-10</td>\n",
       "      <td>3</td>\n",
       "      <td>beautiful but crowded</td>\n",
       "      <td>beautiful but crowded   zion is beautiful but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>759</td>\n",
       "      <td>3</td>\n",
       "      <td>JaniceWriterAuthor</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>5</td>\n",
       "      <td>wow zion is beautiful</td>\n",
       "      <td>wow zion is beautiful   this was our first vis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>759</td>\n",
       "      <td>4</td>\n",
       "      <td>Beachgal003</td>\n",
       "      <td>6</td>\n",
       "      <td>2013-08-29</td>\n",
       "      <td>5</td>\n",
       "      <td>zion national park does it right</td>\n",
       "      <td>zion national park does it right   first the s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   park_id  review_index            reviewer  reviewer_level        date  \\\n",
       "0      759             0      podrozniczka60               6  2015-09-26   \n",
       "1      759             1          prpatel007               1  2013-10-14   \n",
       "2      759             2       Flybob6334580               5  2013-10-10   \n",
       "3      759             3  JaniceWriterAuthor               4  2013-09-19   \n",
       "4      759             4         Beachgal003               6  2013-08-29   \n",
       "\n",
       "   stars                                     title  \\\n",
       "0      3  helpful with your utah vacation planning   \n",
       "1      5                     shame on us americans   \n",
       "2      3                     beautiful but crowded   \n",
       "3      5                     wow zion is beautiful   \n",
       "4      5          zion national park does it right   \n",
       "\n",
       "                                            comments  cluster_assignment  \n",
       "0  helpful with your utah vacation planning   uta...                   0  \n",
       "1  shame on us americans   recently i and my wife...                   0  \n",
       "2  beautiful but crowded   zion is beautiful but ...                   0  \n",
       "3  wow zion is beautiful   this was our first vis...                   0  \n",
       "4  zion national park does it right   first the s...                   0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "park_info_df=pd.read_csv(os.path.join(DATA_FOLDER,CLEAN_PARK_INFO_FILE))\n",
    "park_assignment_df = pd.read_csv(os.path.join(DATA_FOLDER, PARK_ASSIGNMENT_FILE), index_col=0)\n",
    "review_df = pd.read_csv(os.path.join(DATA_FOLDER, CLEAN_REVIEW_FILE), index_col=0)\n",
    "review_df = pd.merge(review_df,park_assignment_df[['park_id','cluster_assignment']],on='park_id')\n",
    "review_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Classification of parks based on every comment using park_id as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''separate parks by cluster assignment label'''\n",
    "def subset_group(cluster_num):   \n",
    "    df = review_df[review_df['cluster_assignment']==cluster_num]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''SGDClassifer'''\n",
    "def pipeline_build(park_weight):\n",
    "    pipeline = Pipeline([('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                                                  max_df = 0.6,\n",
    "                                                  token_pattern = r'\\b\\w+\\b',\n",
    "                                                  ngram_range=(1,1),\n",
    "                                                  min_df=2,\n",
    "                                                  stop_words='english')),\n",
    "                         ('clf', SGDClassifier(penalty='l2',class_weight = park_weight,\n",
    "                                               fit_intercept=True,learning_rate='optimal',\n",
    "                                               shuffle=True,n_iter=500,\n",
    "                                               random_state=0))])\n",
    "    param_grid = {'clf__alpha':[1e-5,1e-4],\n",
    "                  'clf__loss':['hinge','log']\n",
    "                  }\n",
    "\n",
    "\n",
    "    search_grid = GridSearchCV(estimator=pipeline, \n",
    "                               param_grid=param_grid, \n",
    "                               verbose=1,n_jobs=-1,\n",
    "                               refit=True)\n",
    "    return search_grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''get top 20 words with highest coefficient for classification'''\n",
    "def park_features_extract(best_model):\n",
    "    class_label = best_model.named_steps['clf'].classes_\n",
    "    for i,j in enumerate(class_label):\n",
    "        feature_map = best_model.named_steps['vect'].get_feature_names()\n",
    "        wt = best_model.named_steps['clf'].coef_[i]\n",
    "        feature_scores = sorted(zip(feature_map, wt),key=lambda x:x[1],reverse=True)[0:20]\n",
    "        feature_words=[k[0] for k in feature_scores[0:20]]\n",
    "        park_features[j]=feature_words\n",
    "    return park_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''funtions for saving the fitted model or load in fitted model from local''' \n",
    "def save_best_model(CV_search_grid,group_id):\n",
    "     \n",
    "    model_file_name = 'SGD_group_model'+'_'+str(group_id)+'.pkl'\n",
    "    grid_file_name = 'SGD_group_grid'+'_'+str(group_id)+'.pkl'\n",
    "    joblib.dump(CV_search_grid.best_estimator_, os.path.join(MODEL_FOLDER, model_file_name))\n",
    "    joblib.dump(CV_search_grid.grid_scores_,os.path.join(MODEL_FOLDER, grid_file_name))\n",
    "    \n",
    "def load_best_model(file_path, group_id):\n",
    "    model_file_name = 'SGD_group_model'+'_'+str(group_id)+'.pkl'\n",
    "    grid_file_name = 'SGD_group_grid'+'_'+str(group_id)+'.pkl'\n",
    "    best_model = joblib.load(os.path.join(MODEL_FOLDER, model_file_name))\n",
    "    grid_score=joblib.load(os.path.join(MODEL_FOLDER,grid_file_name))\n",
    "    return best_model,grid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_clusters = 15\n",
    "correct_rate = {}\n",
    "park_features = {}\n",
    "\n",
    "if FIT_SWITCH:\n",
    "    for i in range(total_clusters):\n",
    "        subset_df = subset_group(i)\n",
    "        if i!=2: #cluster 2 has too many classes to efficiently implement classification algorithm\n",
    "            park_reviews = subset_df.groupby('park_id', as_index=False).size()\n",
    "            '''pre calculate class weight by park frequency in each cluster '''\n",
    "            park_weight = (park_reviews/float(park_reviews.sum())).to_dict()\n",
    "            '''splitting into train and test dataframe'''\n",
    "            train_f, test_f, train_parkid, test_parkid = train_test_split(subset_df['comments'], \n",
    "                                                                          subset_df['park_id'],\n",
    "                                                                          test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "      \n",
    "            search_grid = pipeline_build(park_weight)\n",
    "            search_grid.fit(train_f,train_parkid)\n",
    "            best_estimator=search_grid.best_estimator_\n",
    "            grid_scores=search_grid.grid_scores_\n",
    "            '''saving fitted model for each cluster'''\n",
    "            save_best_model(search_grid,group_id=i)         \n",
    "            test_results = best_estimator.predict(test_f)\n",
    "            '''calculate classification correct rate'''\n",
    "            correct_rate[i] = np.mean(test_results == test_parkid)\n",
    "            '''get top 20 words with highest coefficient for classification'''\n",
    "            park_features = park_features_extract(best_estimator)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classification_rate_df = pd.DataFrame(correct_rate.items(),columns=['cluster_assignment','correct_rate'])\n",
    "classification_rate_df.to_csv('/Users/Wei.Zhao/Documents/Python code/tripadvisor/class_error.csv')\n",
    "if FIT_SWITCH == True:\n",
    "    feature_df = pd.DataFrame(park_features.items(),columns=['park_id','features'] )\n",
    "    feature_df.to_csv('/Users/Wei.Zhao/Documents/Python code/tripadvisor/park_features.csv')\n",
    "else:\n",
    "    feature_df = pd.read_csv('/Users/Wei.Zhao/Documents/Python code/tripadvisor/park_features.csv',index_col=0)[['park_id','features']]\n",
    "    classitication_rate_df=pd.read_csv('/Users/Wei.Zhao/Documents/Python code/tripadvisor/class_error.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_assignment</th>\n",
       "      <th>correct_rate</th>\n",
       "      <th>cluster_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.489641</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.828521</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.679580</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.689094</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.610309</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.728801</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.668216</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.708766</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.734301</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.600995</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.768607</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.610541</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.703988</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster_assignment  correct_rate  cluster_size\n",
       "0                    0      0.489641            67\n",
       "1                    1      0.828521            71\n",
       "2                    3      0.679580            30\n",
       "3                    4      0.689094            41\n",
       "4                    5      0.610309            42\n",
       "5                    6      0.728801            38\n",
       "6                    7      0.668216            43\n",
       "7                    8      0.708766            29\n",
       "8                    9      0.734301            32\n",
       "9                   10      0.698400            83\n",
       "10                  11      0.600995            62\n",
       "11                  12      0.768607            19\n",
       "12                  13      0.610541            27\n",
       "13                  14      0.703988            41"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df = (review_df[['cluster_assignment','park_id']].drop_duplicates()\n",
    "              .groupby('cluster_assignment',as_index=False)\n",
    "              .count())\n",
    "cluster_df.rename(columns={'park_id':'cluster_size'},inplace=True)\n",
    "pd.merge(classification_rate_df,cluster_df,on='cluster_assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "park_info_assign_df = pd.merge(park_info_df,park_assignment_df, on='park_id')[['park_id','name','park_features']]\n",
    "classification_summary_df = pd.merge(park_info_assign_df,feature_df,on='park_id' )\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "classification_summary_df.rename(columns={'park_features': 'kmeans_tfidf_top_features', 'features': 'classfication features'}, inplace=True)\n",
    "classification_summary_df.head(2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "Trip ski-learn v2",
  "notebookId": 2327273616399586
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
